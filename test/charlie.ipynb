{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import random as rand\n",
    "import sys\n",
    "import collections\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "#from sklearn.metrics.cluster import fowlkes_mallows_score\n",
    "import seaborn as sn\n",
    "import joblib\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(train_data,test_data, pca=False):\n",
    "    '''\n",
    "    Will return arrays of these sizes:\n",
    "        X_train : (752494, 41)\n",
    "        y_train : (752494, )\n",
    "\n",
    "        X_test : (322498, 41)\n",
    "        y_test : (322498, )\n",
    "    '''\n",
    "\n",
    "    # convert all but last column to list of lists for data\n",
    "    X_train = np.array(train_data.iloc[:,:-1].values.tolist())\n",
    "    X_train = StandardScaler().fit_transform(X_train)  # mean of ~0 and variance of 1\n",
    "    # convert last column to list for labels\n",
    "    y_train = np.array(train_data.iloc[:,-1].values.tolist())\n",
    "\n",
    "    # convert all but last column to list of lists for data\n",
    "    X_test = np.array(test_data.iloc[:,:-1].values.tolist())\n",
    "    X_test = StandardScaler().fit_transform(X_test)  # mean of ~0 and variance of 1\n",
    "    # convert last column to list for labels\n",
    "    y_test = np.array(test_data.iloc[:,-1].values.tolist())\n",
    "    \n",
    "   # print(len(y_train))\n",
    "   # print(len(y_test)) \n",
    "    \n",
    "    bin_y_train = []\n",
    "    bin_y_test = []\n",
    "    \n",
    "    for elem in y_train:\n",
    "        if elem == \"normal\":\n",
    "            bin_y_train.append(\"normal\")\n",
    "        else:\n",
    "            bin_y_train.append(\"malicious\")\n",
    "    \n",
    "    for elem in y_test:\n",
    "        if elem == \"normal\":\n",
    "            bin_y_test.append(\"normal\")\n",
    "        else:\n",
    "            bin_y_test.append(\"malicious\")\n",
    "    '''\n",
    "    # pca is optional\n",
    "    desc = \"\"\n",
    "    if pca:\n",
    "        X_train,X_test = pca_data(X_train,X_test)\n",
    "        desc = \"_PCA\"\n",
    "    '''\n",
    "    \n",
    "    bin_y_train = np.array(bin_y_train)\n",
    "    bin_y_test = np.array(bin_y_test)\n",
    "    \n",
    "   # print(len(bin_y_train))\n",
    "   # print(len(bin_y_test))\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)\n",
    "    #np.save(\"X_test%s.npy\" % desc, X_test)\n",
    "   # np.save(\"X_train%s.npy\" % desc, X_test)\n",
    "    np.save(\"y_test_PCA_bin.npy\" , bin_y_test)\n",
    "    np.save(\"y_train_PCA_bin.npy\" , bin_y_train)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_data(X_train,X_test):\n",
    "    pca = PCA()\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    #get variance explained\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    #select what percent var to keep\n",
    "    desired_var = 0.9  # try out different values for this, make graph\n",
    "    #select how many eigenvalues to keep\n",
    "    cumsum = np.cumsum(explained_variance)\n",
    "    k = np.argwhere(cumsum > desired_var)[0]\n",
    "\n",
    "    pca = PCA(n_components=int(k))\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # assume that if x_train doesn't exist, then none of the sets exist\n",
    "    pca = True\n",
    "    if (not os.path.exists('x_test.npy') and not pca) or (not os.path.exists(\"X_test_PCA.npy\") and pca):\n",
    "        train_data = pd.read_csv('../data/train_nodup.csv')\n",
    "        test_data = pd.read_csv('../data/test_nodup.csv')\n",
    "        X_train, y_train, X_test, y_test = split_train_test(train_data,test_data, pca=pca)\n",
    "        print(\"Transformed data\")\n",
    "    else:\n",
    "        desc = \"\"\n",
    "        if pca:\n",
    "            desc = \"_PCA\"\n",
    "        X_train = np.load(\"X_train%s.npy\" % desc)\n",
    "        X_test = np.load(\"X_test%s.npy\" % desc)\n",
    "        y_train = np.load(\"y_train%s.npy\" % desc)\n",
    "        y_test = np.load(\"y_test%s.npy\" % desc)\n",
    "        print(\"Loaded in data\")\n",
    "\n",
    "        \n",
    "    print(len(X_train))\n",
    "    print(len(y_train))\n",
    "    print(X_train[0])\n",
    "    print(y_train[0])\n",
    "    \n",
    "    print(len(X_test))\n",
    "    print(len(y_test))\n",
    "    print(X_test[0])\n",
    "    print(y_test[0])\n",
    "#    result = knn(X_train,X_test,y_train,y_test)\n",
    "#    print(\"KNN F1-Score: %lf\" %(result))\n",
    "#    result = dt(X_train,X_test,y_train,y_test)\n",
    "#    print(\"Decision Tree F1-Score: %lf\" %(result))  \n",
    "#    start = time.time()\n",
    "#    result = sgd(X_train,X_test,y_train,y_test)\n",
    "#    print(\"SGD Classifier (SVM w/ SGD training) F1-Score: %lf\" %(result))\n",
    "#    print(\"SGD Time: %.3lf\" % (time.time() - start))\n",
    "#    result = mlp(X_train,X_test,y_train,y_test)\n",
    "#    print(\"MLP F1-Score: %lf\" %(result))\n",
    "#    result = gnb(X_train,X_test,y_train,y_test)\n",
    "#    print(\"Naive-Bayes F1-Score: %lf\" %(result))\n",
    "#    result = rf(X_train,X_test,y_train,y_test)\n",
    "#    print(\"Random Forest F1-Score: %lf\" %(result))\n",
    "#    start = time.time()\n",
    "#    result = linSVC(X_train,X_test,y_train,y_test)\n",
    "#    print(\"Linear SVC F1-Score: %lf\" %(result))\n",
    "#    print(\"SGD Time: %.3lf\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data\n",
      "1074992\n",
      "1074992\n",
      "[-1.02065973e-01 -2.31551075e-01  1.46726710e+00 -1.07798282e+00\n",
      " -2.90480395e-03 -3.53972864e-03 -4.91800889e-03 -3.09624265e-02\n",
      " -2.35555422e-03 -5.44176230e-02 -9.37337215e-03 -1.30520214e+00\n",
      " -4.33767888e-03 -1.73902059e-02 -9.70544277e-03 -6.98928816e-03\n",
      " -2.04368869e-02 -1.81566006e-02 -5.87933520e-02  0.00000000e+00\n",
      " -1.36399502e-03 -6.17845906e-02  2.60059823e+00  2.38675447e-01\n",
      "  2.07143170e+00  2.06605782e+00 -2.92657624e-01 -2.93070329e-01\n",
      " -1.80886008e+00  3.28466015e-01 -4.44067105e-01  9.09677221e-01\n",
      " -1.26687225e+00 -1.38535555e+00  1.56401221e-01 -4.06204713e-01\n",
      " -3.85643024e-01  2.07118963e+00  2.06943984e+00 -3.03360712e-01\n",
      " -3.00005622e-01]\n",
      "neptune\n",
      "77291\n",
      "77291\n",
      "[-0.0867939   3.43029121  1.18532708  0.62447842 -0.01300763 -0.08457561\n",
      " -0.01079151 -0.03267611 -0.01050797 -0.06747934 -0.07792599 -1.18724293\n",
      " -0.0091851  -0.02833385 -0.00759189 -0.007745   -0.00995278 -0.01297011\n",
      " -0.05248736  0.         -0.0124612  -0.09805102 -0.62884535 -0.30555224\n",
      " -0.33489442 -0.33325986 -0.52164098 -0.51792111  0.63938606 -0.28950614\n",
      " -0.42348742  0.76826904  0.85951133  0.77534301 -0.34468923 -0.25624807\n",
      " -0.29885752 -0.33818071 -0.33419171 -0.53172234 -0.51801083]\n",
      "normal\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt(X_train,X_test,y_train,y_test):\n",
    "    classifier = DecisionTreeClassifier(random_state=0)\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (Original Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(X_train,X_test,y_train,y_test):\n",
    "    RFclas = RandomForestClassifier(max_depth=10, random_state=0,n_estimators=100)\n",
    "    RFclas.fit(X_train,y_train)\n",
    "    y_pred = RFclas.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier (Linear SVC with SGD training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X_train,X_test,y_train,y_test):\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP(NN) (Original Data) -- Note: np.abs() needs to be used on prediction because it guesses negative values sometimes... oh well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(X_train,X_test,y_train,y_test):\n",
    "    bpnn = MLPClassifier(max_iter = 50000) #Very basic BPNN/MLP\n",
    "    bpnn.fit(X_train,y_train)\n",
    "    y_pred = bpnn.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train,X_test,y_train,y_test):\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linSVC(X_train,X_test,y_train,y_test):    \n",
    "    svc = LinearSVC(random_state=0)\n",
    "    svc.fit(X_train,y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnb(X_train,X_test,y_train,y_test): \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
