{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import random as rand\n",
    "import sys\n",
    "import collections\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "#from sklearn.metrics.cluster import fowlkes_mallows_score\n",
    "import seaborn as sn\n",
    "import joblib\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(train_data,test_data, pca=False):\n",
    "    '''\n",
    "    Will return arrays of these sizes:\n",
    "        X_train : (752494, 41)\n",
    "        y_train : (752494, )\n",
    "\n",
    "        X_test : (322498, 41)\n",
    "        y_test : (322498, )\n",
    "    '''\n",
    "\n",
    "    # convert all but last column to list of lists for data\n",
    "    X_train = np.array(train_data.iloc[:,:-1].values.tolist())\n",
    "    X_train = StandardScaler().fit_transform(X_train)  # mean of ~0 and variance of 1\n",
    "    # convert last column to list for labels\n",
    "    y_train = np.array(train_data.iloc[:,-1].values.tolist())\n",
    "\n",
    "    # convert all but last column to list of lists for data\n",
    "    X_test = np.array(test_data.iloc[:,:-1].values.tolist())\n",
    "    X_test = StandardScaler().fit_transform(X_test)  # mean of ~0 and variance of 1\n",
    "    # convert last column to list for labels\n",
    "    y_test = np.array(test_data.iloc[:,-1].values.tolist())\n",
    "    \n",
    "    \n",
    "    # pca is optional\n",
    "    desc = \"\"\n",
    "    if pca:\n",
    "        X_train,X_test = pca_data(X_train,X_test)\n",
    "        desc = \"_PCA\"\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)\n",
    "    np.save(\"X_test%s.npy\" % desc, X_test)\n",
    "   # np.save(\"X_test%s.npy\" % desc, X_test)\n",
    "    np.save(\"y_test%s.npy\" % desc, y_test)\n",
    "   # np.save(\"y_test%s.npy\" % desc, y_test)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_data(X_train,X_test):\n",
    "    pca = PCA()\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    #get variance explained\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    #select what percent var to keep\n",
    "    desired_var = 0.9  # try out different values for this, make graph\n",
    "    #select how many eigenvalues to keep\n",
    "    cumsum = np.cumsum(explained_variance)\n",
    "    k = np.argwhere(cumsum > desired_var)[0]\n",
    "\n",
    "    pca = PCA(n_components=int(k))\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # assume that if x_train doesn't exist, then none of the sets exist\n",
    "    pca = True\n",
    "    if (not os.path.exists('x_test.npy') and not pca) or (not os.path.exists(\"X_test_PCA.npy\") and pca):\n",
    "        train_data = pd.read_csv('../data/train_nodup.csv')\n",
    "        test_data = pd.read_csv('../data/test_nodup.csv')\n",
    "        X_train, y_train, X_test, y_test = split_train_test(train_data,test_data, pca=pca)\n",
    "        print(\"Transformed data\")\n",
    "    else:\n",
    "        desc = \"\"\n",
    "        if pca:\n",
    "            desc = \"_PCA\"\n",
    "        X_train = np.load(\"X_train%s.npy\" % desc)\n",
    "        X_test = np.load(\"X_test%s.npy\" % desc)\n",
    "        y_train = np.load(\"y_train%s.npy\" % desc)\n",
    "        y_test = np.load(\"y_test%s.npy\" % desc)\n",
    "        print(\"Loaded in data\")\n",
    "\n",
    "        \n",
    "    print(len(X_train))\n",
    "    print(len(y_train))\n",
    "    print(X_train[0])\n",
    "    print(y_train[0])\n",
    "    \n",
    "    print(len(X_test))\n",
    "    print(len(y_test))\n",
    "    print(X_test[0])\n",
    "    print(y_test[0])\n",
    "#    result = knn(X_train,X_test,y_train,y_test)\n",
    "#    print(\"KNN F1-Score: %lf\" %(result))\n",
    "#    result = dt(X_train,X_test,y_train,y_test)\n",
    "#    print(\"Decision Tree F1-Score: %lf\" %(result))  \n",
    "#    start = time.time()\n",
    "#    result = sgd(X_train,X_test,y_train,y_test)\n",
    "#    print(\"SGD Classifier (SVM w/ SGD training) F1-Score: %lf\" %(result))\n",
    "#    print(\"SGD Time: %.3lf\" % (time.time() - start))\n",
    "#    result = mlp(X_train,X_test,y_train,y_test)\n",
    "#    print(\"MLP F1-Score: %lf\" %(result))\n",
    "#    result = gnb(X_train,X_test,y_train,y_test)\n",
    "#    print(\"Naive-Bayes F1-Score: %lf\" %(result))\n",
    "#    result = rf(X_train,X_test,y_train,y_test)\n",
    "#    print(\"Random Forest F1-Score: %lf\" %(result))\n",
    "#    start = time.time()\n",
    "#    result = linSVC(X_train,X_test,y_train,y_test)\n",
    "#    print(\"Linear SVC F1-Score: %lf\" %(result))\n",
    "#    print(\"SGD Time: %.3lf\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data\n",
      "1074992\n",
      "1074992\n",
      "[ 5.75126242e+00 -1.70509318e+00 -5.33800714e-01  4.09963237e-01\n",
      " -8.41918805e-02 -9.81417699e-03  2.83175681e-01 -2.55221073e-01\n",
      " -7.92219459e-02 -4.84546833e-02  1.83756603e-02  6.50642274e-03\n",
      "  9.54654281e-03 -5.26766418e-03 -3.27437041e-02 -6.56413875e-02\n",
      " -1.92198816e-02  1.26038534e-01  2.43348913e-01 -8.01061798e-02]\n",
      "neptune\n",
      "77291\n",
      "77291\n",
      "[-0.8918304  -0.93719466  1.09077414 -1.00292607 -0.47380975  1.39790852\n",
      " -0.25914386  0.35980159  0.46438818 -0.49207928  0.20906435 -0.063602\n",
      " -0.1026246   0.03688954  0.12224632  0.09845588  0.12020273  1.12370159\n",
      " -1.33548419  0.26763392]\n",
      "normal\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt(X_train,X_test,y_train,y_test):\n",
    "    classifier = DecisionTreeClassifier(random_state=0)\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (Original Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(X_train,X_test,y_train,y_test):\n",
    "    RFclas = RandomForestClassifier(max_depth=10, random_state=0,n_estimators=100)\n",
    "    RFclas.fit(X_train,y_train)\n",
    "    y_pred = RFclas.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier (Linear SVC with SGD training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X_train,X_test,y_train,y_test):\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP(NN) (Original Data) -- Note: np.abs() needs to be used on prediction because it guesses negative values sometimes... oh well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(X_train,X_test,y_train,y_test):\n",
    "    bpnn = MLPClassifier(max_iter = 50000) #Very basic BPNN/MLP\n",
    "    bpnn.fit(X_train,y_train)\n",
    "    y_pred = bpnn.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train,X_test,y_train,y_test):\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linSVC(X_train,X_test,y_train,y_test):    \n",
    "    svc = LinearSVC(random_state=0)\n",
    "    svc.fit(X_train,y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnb(X_train,X_test,y_train,y_test): \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
