{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import random as rand\n",
    "import sys\n",
    "import collections\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "#from sklearn.metrics.cluster import fowlkes_mallows_score\n",
    "import seaborn as sn\n",
    "import joblib\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, pca=False):\n",
    "    '''\n",
    "    Will return arrays of these sizes:\n",
    "        X_train : (752494, 41)\n",
    "        y_train : (752494, )\n",
    "\n",
    "        X_test : (322498, 41)\n",
    "        y_test : (322498, )\n",
    "    '''\n",
    "\n",
    "    # convert all but last column to list of lists for data\n",
    "    X = np.array(data.iloc[:,:-1].values.tolist())\n",
    "    X = StandardScaler().fit_transform(X)  # mean of ~0 and variance of 1\n",
    "    # convert last column to list for labels\n",
    "    y = np.array(data.iloc[:,-1].values.tolist())\n",
    "\n",
    "    # pca is optional\n",
    "    desc = \"\"\n",
    "    if pca:\n",
    "        X = pca_data(X)\n",
    "        desc = \"_PCA\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)\n",
    "    np.save(\"X_train%s.npy\" % desc, X_train)\n",
    "    np.save(\"X_test%s.npy\" % desc, X_test)\n",
    "    np.save(\"y_train%s.npy\" % desc, y_train)\n",
    "    np.save(\"y_test%s.npy\" % desc, y_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_data(X):\n",
    "    pca = PCA()\n",
    "    X = pca.fit_transform(X)\n",
    "\n",
    "    #get variance explained\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    '''\n",
    "    #make first plot of just principal components\n",
    "    fig1 = plt.figure()\n",
    "    plt.plot(explained_variance)\n",
    "    plt.title(\"Principal Components\")\n",
    "    plt.ylabel(\"Percent of Variance Explained\")\n",
    "    plt.xlabel(\"Principal Component\")\n",
    "    plt.savefig(\"graphs/principal_comp_.png\")\n",
    "    '''\n",
    "    #select what percent var to keep\n",
    "    desired_var = 0.9  # try out different values for this, make graph\n",
    "    #select how many eigenvalues to keep\n",
    "    cumsum = np.cumsum(explained_variance)\n",
    "    k = np.argwhere(cumsum > desired_var)[0]\n",
    "    '''\n",
    "    #make second plot of cum var explained\n",
    "    fig2 = plt.figure()\n",
    "    plt.plot(cumsum)\n",
    "    plt.title(\"Variance Explained\")\n",
    "    plt.plot(k, cumsum[k], 'ro', label=\"Eigenvalue #%d with %.2f Variance\" % (k, desired_var))\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Cumulative Percent of Variance Explained\")\n",
    "    plt.xlabel(\"Principal Component\")\n",
    "    plt.savefig(\"graphs/var_exp_.png\")\n",
    "    '''\n",
    "    pca = PCA(n_components=int(k))\n",
    "    X = pca.fit_transform(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # assume that if x_train doesn't exist, then none of the sets exist\n",
    "    pca = True\n",
    "    if (not os.path.exists('x_train.npy') and not pca) or (not os.path.exists(\"X_train_PCA.npy\") and pca):\n",
    "        data = pd.read_csv('../data/train_nodup.csv')\n",
    "        X_train, X_test, y_train, y_test = split_train_test(data, pca=pca)\n",
    "        print(\"Transformed data\")\n",
    "    else:\n",
    "        desc = \"\"\n",
    "        if pca:\n",
    "            desc = \"_PCA\"\n",
    "        X_train = np.load(\"X_train%s.npy\" % desc)\n",
    "        X_test = np.load(\"X_test%s.npy\" % desc)\n",
    "        y_train = np.load(\"y_train%s.npy\" % desc)\n",
    "        y_test = np.load(\"y_test%s.npy\" % desc)\n",
    "        print(\"Loaded in data\")\n",
    "\n",
    "#    result = knn(X_train,X_test,y_train,y_test)\n",
    "#    print(\"KNN F1-Score: %lf\" %(result))\n",
    "#    result = dt(X_train,X_test,y_train,y_test)\n",
    "#    print(\"Decision Tree F1-Score: %lf\" %(result))  \n",
    "    start = time.time()\n",
    "    result = sgd(X_train,X_test,y_train,y_test)\n",
    "    print(\"SGD Classifier (SVM w/ SGD training) F1-Score: %lf\" %(result))\n",
    "    print(\"SGD Time: %.3lf\" % (time.time() - start))\n",
    "#    result = mlp(X_train,X_test,y_train,y_test)\n",
    "#    print(\"MLP F1-Score: %lf\" %(result))\n",
    "#    result = gnb(X_train,X_test,y_train,y_test)\n",
    "#    print(\"Naive-Bayes F1-Score: %lf\" %(result))\n",
    "#    result = rf(X_train,X_test,y_train,y_test)\n",
    "#    print(\"Random Forest F1-Score: %lf\" %(result))\n",
    "    start = time.time()\n",
    "    result = linSVC(X_train,X_test,y_train,y_test)\n",
    "    print(\"Linear SVC F1-Score: %lf\" %(result))\n",
    "    print(\"SGD Time: %.3lf\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in data\n",
      "SGD Classifier (SVM w/ SGD training) F1-Score: 0.987635\n",
      "SGD Time: 30.137\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt(X_train,X_test,y_train,y_test):\n",
    "    classifier = DecisionTreeClassifier(random_state=0)\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (Original Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(X_train,X_test,y_train,y_test):\n",
    "    RFclas = RandomForestClassifier(max_depth=10, random_state=0,n_estimators=100)\n",
    "    RFclas.fit(X_train,y_train)\n",
    "    y_pred = RFclas.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier (Linear SVC with SGD training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X_train,X_test,y_train,y_test):\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP(NN) (Original Data) -- Note: np.abs() needs to be used on prediction because it guesses negative values sometimes... oh well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(X_train,X_test,y_train,y_test):\n",
    "    bpnn = MLPClassifier(max_iter = 50000) #Very basic BPNN/MLP\n",
    "    bpnn.fit(X_train,y_train)\n",
    "    y_pred = bpnn.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train,X_test,y_train,y_test):\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linSVC(X_train,X_test,y_train,y_test):    \n",
    "    svc = LinearSVC(random_state=0)\n",
    "    svc.fit(X_train,y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnb(X_train,X_test,y_train,y_test): \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
